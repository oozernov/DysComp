resABCD[1,] <- c(dfABCD[i,1],as.numeric(dfABCD[i,2]),'rp',k-9,on,off)    }
else {
resABCD[nrow(resABCD) + 1,] <- c(dfABCD[i,1],as.numeric(dfABCD[i,2]),'rp',k-9,on,off)
}
}
}
d_final<-merge(resABCD,group,"id")
View(d_final)
resABCD$subgroup<-NULL
resABCD<-merge(resABCD,group,"id")
View(resABCD)
resABCD$On <- as.numeric(resABCD$On)
resABCD$Off <- as.numeric(resABCD$Off)
resABCD$trial <- as.numeric(resABCD$trial)
res4Plot <- ddply(resABCD,.(subgroup.x,trial),summarize,On = mean(as.numeric(On)),Off = mean(as.numeric(Off)),Prop_On = mean(as.numeric(On)/as.numeric(Off+On)))
Packages <- c("dplyr", "stats", "nFactors", "psych", "ggplot2", "lme4",
"gridExtra", "dplyr","caret","tidyverse", "ggplot2","plyr")
res4Plot <- ddply(resABCD,.(subgroup.x,trial),summarize,On = mean(as.numeric(On)),Off = mean(as.numeric(Off)),Prop_On = mean(as.numeric(On)/as.numeric(Off+On)))
??ddply
res4Plot <- apply(resABCD,.(subgroup.x,trial),summarize,On = mean(as.numeric(On)),Off = mean(as.numeric(Off)),Prop_On = mean(as.numeric(On)/as.numeric(Off+On)))
library(plyr)
res4Plot <- ddply(resABCD,.(subgroup,trial),summarize,On = mean(as.numeric(On)),Off = mean(as.numeric(Off)),Prop_On = mean(as.numeric(On)/as.numeric(Off+On)))
e3=rnorm(length(res4Plot$trial), 3)
ggplot(data = res4Plot, aes(x=as.numeric(trial), y=Prop_On, color = subgroup)) + geom_line() +geom_point()  + geom_errorbar(aes(x=as.numeric(trial), y= Prop_On, ymin=Prop_On-e3, ymax = Prop_On+e3), width=0.1)+scale_x_continuous(limits = c(3, 9))
ggplot(data = res4Plot, aes(x=as.numeric(trial), y=Prop_On, color = subgroup)) + geom_line() +geom_point()  + geom_errorbar(aes(x=as.numeric(trial), y= Prop_On, ymin=Prop_On, ymax = Prop_On), width=0.1)+scale_x_continuous(limits = c(3, 9))
ggplot(data = res4Plot, aes(x=as.numeric(trial), y=Prop_On, color = subgroup))+
geom_line() +geom_point()  +
geom_bar(stat="identity",color="black",position=position_dodge())+
geom_errorbar(aes(ymin=Prop_On-sd,ymax=Prop_On+sd),
width=.2, position=position_dodge(.9))
scale_x_continuous(limits = c(3, 9))
ggplot(data = res4Plot, aes(x=as.numeric(trial), y=Prop_On, color = subgroup))+
geom_line() +geom_point()  +
geom_bar(stat="identity",color="black",position=position_dodge())+
geom_errorbar(aes(ymin=Prop_On-sd,ymax=Prop_On+sd),
width=.2, position=position_dodge(.9))
res4Plot <- plyr::ddply(resABCD,.(subgroup,trial),summarize,On = mean(as.numeric(On)),Off = mean(as.numeric(Off)),Prop_On = mean(as.numeric(On)/as.numeric(Off+On),sd=sd(Prop_On)))
res4Plot$sd
res4Plot$sd<-sd(res4Plot$Prop_On)
res4Plot$sd
# Ola Ozernov-Palchik
#
# Updated-Aug 2 2019
#
# MIT Speech Perception study, Tone Anchoring Task
#
#### Setup ####
setwd("~/Dropbox (MIT)/Com_Dys_2016_data/final_for_sharing/final_code_JT")
#PerrachioneLab <- dir("/Volumes", pattern = "PerrachioneLab|sar-research", full.names = TRUE)
Packages <- c("dplyr", "readr", "magrittr", "tidyr", "ggplot2", "lme4", "lmerTest",
"emmeans", "sjstats","dabestr","gridExtra")
lapply(Packages, library, character.only = TRUE)
#This code is specific to PerrachioneLab
#source(file.path(PerrachioneLab, "software", "r-scripts", "load_packages.R"))
#load_packages("dplyr", "readr", "magrittr", "tidyr", "ggplot2", "lme4", "lmerTest",
#"emmeans", "sjstats")
#### Load and organize the data ####
# adult data
adult <- read_csv("data/tonethres_data_a_010719.csv")
groups_a <- read_csv("data/adult_groups_012418.csv") %>%
mutate(group = as.factor(group)) %>%
rename(PartID = Subject)
adult %<>%
rename(PartID = Subject) %>%
inner_join(groups_a, by = c("PartID")) %>% # add DD columns to the data
select(-X1)
# child data
child <- read_csv("data/child_tonethres_data_122718.csv")
groups_c <- read.csv("data/groups_041817.csv") %>%
mutate(group = ifelse(Typical == 1, "Typ", "Dys")) %>%
select(-DD, -Typical)
child %<>%
inner_join(groups_c, by = "PartID") %>% # add DD columns to the data
mutate(group = as.factor(group)) %>%
select(-Subject)
# combine the datasets
d <- bind_rows("Adult" = adult,
"Child" = child,
.id = "age") %>%
mutate(com_cond = paste(group, "-", age),
# rename condition to standard/no-standard
cond = ifelse(cond == "a", "Standard", "No-standard"))
groups <- bind_rows("Adult" = groups_a,
"Child" = groups_c,
.id = "age")
d_m = d %>%
dplyr::group_by(PartID, cond)%>%
dplyr::summarise(mean_rt = mean(RT, na.rm = T))%>%
left_join(groups, by = c("PartID"))
m2=(lmer(mean_rt~cond*group*age+(1|PartID), data=d_m, REML=TRUE))
anova(m2)
lsmeans(m2, list(pairwise ~ cond|group), adjust = "tukey")#RT varies in Dys, but not Typ as function of context
lsmeans(m2, list(pairwise ~ age), adjust = "tukey") #children slower
d_m = d %>%
dplyr::group_by(PartID, cond)%>%
dplyr::summarise(mean_rt = mean(RT, na.rm = T))%>%
left_join(groups, by = c("PartID"))
# combine the datasets
d <- bind_rows("Adult" = adult,
"Child" = child,
.id = "age") %>%
mutate(com_cond = paste(group, "-", age),
# rename condition to standard/no-standard
cond = ifelse(cond == "a", "Standard", "No-standard"))
child %<>%
inner_join(groups_c, by = "PartID") %>% # add DD columns to the data
mutate(group = as.factor(group)) %>%
select(-Subject)
setwd("~/Dropbox (MIT)/Com_Dys_2016_data/final_for_sharing/final_code_JT")
#### Load and organize the data ####
# adult data
adult <- read_csv("data/tonethres_data_a_010719.csv")
groups_a <- read_csv("data/adult_groups_012418.csv") %>%
mutate(group = as.factor(group)) %>%
rename(PartID = Subject)
adult <- read_csv("data/tonethres_data_a_010719.csv")
groups_a <- read_csv("data/adult_groups_012418.csv") %>%
mutate(group = as.factor(group)) %>%
rename(PartID = Subject)
groups_a <- read_csv("data/adult_groups_012418.csv") %>%
mutate(group = as.factor(group)) %>%
rename(PartID = Subject)
groups_a <- read_csv("data/adult_groups_012418.csv")
groups_a <- read_csv("data/adult_groups_012418.csv") %>%
dplyr::mutate(group = as.factor(group)) %>%
rename(PartID = Subject)
groups_a <- read_csv("data/adult_groups_012418.csv") %>%
dplyr::mutate(group = as.factor(group)) %>%
dplyr::rename(PartID = Subject)
View(groups_a)
adult %<>%
dplyr::rename(PartID = Subject) %>%
inner_join(groups_a, by = c("PartID")) %>% # add DD columns to the data
select(-X1)
groups_c <- read.csv("data/groups_041817.csv") %>%
mutate(group = ifelse(Typical == 1, "Typ", "Dys")) %>%
select(-DD, -Typical)
groups_c <- read.csv("data/groups_041817.csv") %>%
dplyr::mutate(group = ifelse(Typical == 1, "Typ", "Dys")) %>%
dplyr:: select(-DD, -Typical)
child %<>%
inner_join(groups_c, by = "PartID") %>% # add DD columns to the data
dplyr:: mutate(group = as.factor(group)) %>%
dplyr:: select(-Subject)
# combine the datasets
d <- bind_rows("Adult" = adult,
"Child" = child,
.id = "age") %>%
mutate(com_cond = paste(group, "-", age),
# rename condition to standard/no-standard
cond = ifelse(cond == "a", "Standard", "No-standard"))
groups <- bind_rows("Adult" = groups_a,
"Child" = groups_c,
.id = "age")
d_m = d %>%
dplyr::group_by(PartID, cond)%>%
dplyr::summarise(mean_rt = mean(RT, na.rm = T))%>%
left_join(groups, by = c("PartID"))
m2=(lmer(mean_rt~cond*group*age+(1|PartID), data=d_m, REML=TRUE))
anova(m2)
lsmeans(m2, list(pairwise ~ cond|group), adjust = "tukey")#RT varies in Dys, but not Typ as function of context
lsmeans(m2, list(pairwise ~ age), adjust = "tukey") #children slower
m2=(lmer(mean_rt~cond*group*age+(1|PartID), data=d_m, REML=TRUE))
View(d_m)
str(d_m)
d_m$group<-as.factor(d_m$group)
str(d_m)
d_m$age<-as.factor(d_m$age)
d_m$group<-as.factor(d_m$group)
d_m$cond<-as.factor(d_m$cond)
m2=(lmer(mean_rt~cond*group*age+(1|PartID), data=d_m, REML=TRUE))
str(d_m)
# tone.R
# Ola Ozernov-Palchik
#
# Updated-Aug 2 2019
#
# MIT Speech Perception study, Tone Anchoring Task
#
#### Setup ####
setwd("~/Dropbox (MIT)/Com_Dys_2016_data/final_for_sharing/final_code_JT")
#PerrachioneLab <- dir("/Volumes", pattern = "PerrachioneLab|sar-research", full.names = TRUE)
Packages <- c("dplyr", "readr", "magrittr", "tidyr", "ggplot2", "lme4", "lmerTest",
"emmeans", "sjstats","dabestr","gridExtra")
lapply(Packages, library, character.only = TRUE)
#This code is specific to PerrachioneLab
#source(file.path(PerrachioneLab, "software", "r-scripts", "load_packages.R"))
#load_packages("dplyr", "readr", "magrittr", "tidyr", "ggplot2", "lme4", "lmerTest",
#"emmeans", "sjstats")
#### Load and organize the data ####
# adult data
adult <- read_csv("data/tonethres_data_a_010719.csv")
groups_a <- read_csv("data/adult_groups_012418.csv") %>%
dplyr::mutate(group = as.factor(group)) %>%
dplyr::rename(PartID = Subject)
adult %<>%
dplyr::rename(PartID = Subject) %>%
inner_join(groups_a, by = c("PartID")) %>% # add DD columns to the data
select(-X1)
# child data
child <- read_csv("data/child_tonethres_data_122718.csv")
groups_c <- read.csv("data/groups_041817.csv") %>%
dplyr::mutate(group = ifelse(Typical == 1, "Typ", "Dys")) %>%
dplyr:: select(-DD, -Typical)
child %<>%
inner_join(groups_c, by = "PartID") %>% # add DD columns to the data
dplyr:: mutate(group = as.factor(group)) %>%
dplyr:: select(-Subject)
# combine the datasets
d <- bind_rows("Adult" = adult,
"Child" = child,
.id = "age") %>%
mutate(com_cond = paste(group, "-", age),
# rename condition to standard/no-standard
cond = ifelse(cond == "a", "Standard", "No-standard"))
cond = ifelse(cond == "a", "Standard", "No-standard"))
groups <- bind_rows("Adult" = groups_a,
"Child" = groups_c,
.id = "age")
d_m = d %>%
dplyr::group_by(PartID, cond)%>%
dplyr::summarise(mean_rt = mean(RT, na.rm = T))%>%
left_join(groups, by = c("PartID"))
d_m$age<-as.factor(d_m$age)
d_m$group<-as.factor(d_m$group)
d_m$cond<-as.factor(d_m$cond)
m2=(lmer(mean_rt~cond*group*age+(1|PartID), data=d_m, REML=TRUE))
anova(m2)
eta_sq(m2)
lsmeans(m2, list(pairwise ~ age), adjust = "tukey") #children slower
# phon_rest.R
# Ola Ozernov-Palchik
# Jul 2019
#2 Jan 2019
#
# MIT Speech Perception study, Phonemic Restoration
#
#### Setup ####
setwd("~/Dropbox (MIT)/Com_Dys_2016_data/final_for_sharing/final_code_JT")
Packages <- c("dplyr", "readr", "magrittr", "tidyr", "ggplot2",
"lme4", "lmerTest","emmeans", "sjstats", "plotrix","dabestr","lmerTest",
"lmPerm","gridExtra", "grid","ggpubr")
lapply(Packages, library, character.only = TRUE)
### Load and organize the data ####
# child data
d_c <- read_csv("data/child_phonresto_data_122718.csv")
groups_c <- read_csv("data/groups_041817.csv")
d_c %<>%
# add group columns to the dataframe
inner_join(groups_c, by = "PartID") %>%
# make columns to distinguish condition and word set (i.e. bag-tag)
separate(cond, into = c("pres", "condition"), sep = "-", remove = FALSE) %>%
separate(soundfile, into = c("word1", "word2", "pres1", "condition1"), sep = "-", remove = FALSE) %>%
mutate(word_set = paste(word1, word2, sep = "-"),
item = paste(word_set, condition, sep = "-"),
# calculate correct when missing
correct = ifelse(is.na(correct),
ifelse(substr(cond, 1, 1) == substr(response,
nchar(response),
nchar(response)),
1, 0),
correct)) %>%
select(-pres, -word1, -word2, -pres1, -condition1)
# remove bad items (based on adult mturk data)
bad_items <- c('brown-crown', 'drill-grill', 'pear-bear', 'pole-goal')
d_c %<>%
filter(!word_set %in% bad_items) %>%
na.omit()
# child sample size
counts <- d_c %>%
group_by(PartID) %>%
summarize(m = mean(keyRT)) %>%
left_join(groups_c, by = "PartID") %>%
mutate(group = ifelse(DD == 1, "Dys", "Typ"))
count(counts, group)
# adult data
d_a <- read_csv("data/phonresto_data_a_010719.csv")
groups_a <- read_csv("data/adult_groups_012418.csv")
key <- read_csv("data/phonresto_key.csv") %>%
select(soundfile, correct)
d_a %<>%
# add group columns to the dataframe
inner_join(groups_a, by = "Subject") %>%
# make columns to distinguish condition and word set (i.e. bag-tag)
separate(cond, into = c("condition", "pres"), sep = "-", remove = FALSE) %>%
separate(soundfile, into = c("word", "condition1", "pres1"), sep = "-", remove = FALSE) %>%
mutate(item = paste(word, condition1, sep = "-")) %>%
# get rid of columns we don't need
select(-pres, -word, -pres1, -condition1, -X1) %>%
# add correct column
left_join(key, by = "soundfile") %>%
mutate(is_correct = ifelse(response == correct, 1, 0))
# adult sample size
counts <- d_a %>%
group_by(Subject) %>%
summarize(m = mean(keyRT)) %>%
left_join(groups_a, by = "Subject")
count(counts, group)
d_c2<-d_c%>%filter(condition!='n')%>%
dplyr::group_by(PartID,condition) %>%
dplyr::summarize(mean.rti = mean(keyRT, na.rm = TRUE))%>%
ungroup() %>%
left_join(groups_c, by = c("PartID"))
d_c2$group<-ifelse(d_c2$DD==1,"Dys","Typ")
d_c2$condition <- ifelse(d_c2$condition=="c","Cong","Incong")
#adult
d_a2<-d_a%>%dplyr::group_by(Subject,condition) %>%
dplyr::summarize(mean.rti = mean(keyRT, na.rm = TRUE))%>%
ungroup() %>%
left_join(groups_a, by = c("Subject"))
d_a2$condition <- ifelse(d_a2$condition=="cc","Cong","Incong")
d_a2$group_cond<-paste(d_a2$group, "-", d_a2$condition)
m1<-lmerTest::lmer(mean.rti~condition*group+(1|Subject), data=d_a2,REML=TRUE)
anova(m1)
lsmeans(m1, list(pairwise ~ group), adjust = "tukey")
lsmeans(m1, list(pairwise ~ condition), adjust = "tukey")
d_c$DD<-as.factor(d_c$DD)
d_c$condition<-as.factor(d_c$condition)
m2<-lmer(mean.rti~condition*group+(1|PartID), data=d_c2,REML=TRUE)
anova(m2)
eta_sq(m1)
lsmeans(m2, list(pairwise ~ condition), adjust = "tukey") #highest RT in Cong, Lowest in N
?
dabestr
??dabestr
install.packages("neuropsychology")
Packages <- c("dplyr", "reshape", "magrittr", "tidyr", "ggplot2",
"lme4", "lmerTest","emmeans", "sjstats", "plotrix","dabestr","lmerTest",
"lmPerm","gridExtra", "grid","ggpubr")
lapply(Packages, library, character.only = TRUE)
install.packages("reshape")
Packages <- c("dplyr", "reshape", "magrittr", "tidyr", "ggplot2",
"lme4", "lmerTest","emmeans", "sjstats", "plotrix","dabestr","lmerTest",
"lmPerm","gridExtra", "grid","ggpubr")
lapply(Packages, library, character.only = TRUE)
install.packages("emmeans")
setwd("~/Dropbox (MIT)/Com_Dys_2016_data/final_for_sharing/final_code_JT/data")
adult = read.csv("phonwords_data_a_010719.csv", header = T, na.strings = c("", "NA"))
adult = read.csv("phonwords_data_a_010719.csv", header = T, na.strings = c("", "NA"))
#exclude the 'd' continuum from adult dataset
adult=adult %>%
filter(grepl("gi", tolower(soundfile)))
groups_a = read.csv("adult_groups_012418.csv", header = T, na.strings = c("", "NA"))
## load child data
setwd("Com_Dys_2016_data")
child = read.csv("child_phonwords_data_122718.csv", header = T, na.strings = c("", "NA"))
groups_c = read.csv("groups_041817.csv", header = T, na.strings = c("", "NA"))
# add DD columns to the data
child = merge(child, groups_c, by = "PartID")
adult = merge(adult, groups_a, by = "Subject")
names(adult)[names(adult) == 'Subject'] <- 'PartID'
child$group<-ifelse(child$Typical==1, "Typ","Dys")
child$group<-as.factor(child$group)
adult$X <- NULL
adult$X.y <- NULL
adult$X.1 <- NULL
child$Subject <- NULL
child$DD <- NULL
child$Typical <- NULL
##combine the datasets
adult$age<-"Adult"
child$age<-"Child"
d = bind_rows(adult, child) #changed from 'combine' to 'bind_rows'
groups_c$group = ifelse(groups_c$Typical==1, "Typ","Dys")
groups_a$X <- NULL
groups_a$X.1 <- NULL
groups_c$DD <- NULL
groups_c$Typical <- NULL
#groups_a <- rename(groups_a, c("Subject" = "PartID"))
names(groups_a)[names(groups_a) == 'Subject'] <- 'PartID'
groups = bind_rows(groups_a, groups_c)
#d$PartID = as.factor(d$PartID)
# make new columns that will let us separate by continuum
new_columns = colsplit(d$soundfile, "-", names=c("word", "rest"))
d = cbind(d, new_columns)
new_columns = colsplit(d$rest, "", names=c("n_step","w", "x", "y", "z"))
d = cbind(d, new_columns)
d=d%>%filter(!(PartID %in% c('READ_6103')))
# get rid of columns we don't need
d$rest <- NULL
d$w <- NULL
d$x <- NULL
d$y <- NULL
d$z <- NULL
d$com_cond = paste(d$group, "-", d$age)
# define some vectors to use for graphs later
x_ticks = c(1,2,3,4,5,6,7)
y_ticks = c(0,10,20,30,40,50,60,70,80,90,100)
y_new_ticks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
####################### Raw percentage /g/ response ###############
###################### Raw percentage /g/ response ########################
# d$word<-NULL #had a duplication error for 'word'
d_g = d %>% filter(word %in% c('gift', 'gith', 'giss'))
d_g$phoneme_g = ifelse(d_g$response == 'G', 1, 0)
d_conts = d_g %>% dplyr::group_by(com_cond, word, n_step) %>%
dplyr::summarise(percent_g = mean(phoneme_g, na.rm = T))
d_conts$word_pair = ifelse(d_conts$word == "gift", "gift-kift",
ifelse(d_conts$word == "gith", "gith-kith", "giss-kiss"))
ggplot(d_conts, aes(colour=word_pair, y=percent_g*100, x=n_step)) +
geom_point() + geom_line() +
scale_x_continuous(breaks = x_ticks, limits = c(1, 7)) +
scale_y_continuous(breaks = y_ticks, limits = c(0, 100)) +
labs(x = "Step", y = "% /g/ response", title = "Raw /g/ Response") +
theme_bw() +
theme(panel.grid.minor = element_blank(),
panel.background = element_rect(fill = 'white'),
legend.position = "right",
plot.title = element_text(hjust = 0.5)) +
facet_wrap(~com_cond, ncol = 2)
########################## Get sample sizes by group ##########################
counts = d %>% dplyr::group_by(PartID, age) %>%
dplyr::summarise(m = mean(n_step))
counts = merge(counts, groups, by = "PartID")
dplyr::count(counts, age, group)
##### 6. Create d_export #####
d_export = d_g %>% filter(word == 'gith') %>%
dplyr::group_by(PartID, n_step, group, age) %>%
dplyr::summarise(m_g = mean(phoneme_g, na.rm = T))
d_export = distinct(data.frame(d_export[, c("PartID")]))
names(d_export)=c("PartID")
##### 7. glm logistic regression to extract slope/inflection #####
calc_g = d_g %>%
dplyr::group_by(PartID, word, n_step, group, age, com_cond)
#dplyr::summarise(prop_g = mean(phoneme_g, na.rm = T))
d_glm_fit = data.frame(PartID = rep(0, 10),
word = rep(0, 10),
group = rep(0, 10),
age = rep(0, 10),
com_cond = rep(0, 10),
slope_coef = rep(0, 10),
inflection_step = rep(0, 10),
value_at_gith_i = rep(0, 10),
deviance = rep(0, 10))
index = 0
for (sub in d_export$PartID) {
index = index + 1
temp = calc_g %>% filter(PartID == sub)
temp_gift = temp %>% filter(word == "gift")
temp_gith = temp %>% filter(word == "gith")
temp_giss = temp %>% filter(word == "giss")
temp_group = as.character(temp$group[1])
temp_age = as.character(temp$age[1])
temp_com_cond = as.character(temp$com_cond[1])
model_gift <- glm(phoneme_g ~ n_step, family = binomial(link = 'logit'), data = temp_gift)
model_gith <- glm(phoneme_g ~ n_step, family = binomial(link = 'logit'), data = temp_gith)
model_giss <- glm(phoneme_g ~ n_step, family = binomial(link = 'logit'), data = temp_giss)
b_gith = as.numeric(model_gith$coefficients[1])
m_gith = as.numeric(model_gith$coefficients[2])
i_gith = -b_gith/m_gith #-intercept/slope
dev_fit_gith = as.numeric(deviance(model_gith))
gith_x = data.frame('n_step' = i_gith)
value_gith_frame = data.frame(gith_x$n_step, predict(model_gith, list(n_step = gith_x$n_step), type = 'response'))
value_gith = value_gith_frame[[2]]
new2 = c(sub, "gith", temp_group, temp_age, temp_com_cond, m_gith, i_gith, value_gith, dev_fit_gith)
d_glm_fit[index, ] = new2
index = index + 1
b_gift = as.numeric(model_gift$coefficients[1])
m_gift = as.numeric(model_gift$coefficients[2])
i_gift = -b_gift/m_gift
dev_fit_gift = as.numeric(deviance(model_gift))
value_gift_frame = data.frame(gith_x$n_step, predict(model_gift, list(n_step = gith_x$n_step), type = 'response'))
value_gift = value_gift_frame[[2]]
new1 = c(sub, "gift", temp_group, temp_age, temp_com_cond, m_gift, i_gift, value_gift, dev_fit_gift)
d_glm_fit[index, ] = new1
index = index + 1
b_giss = as.numeric(model_giss$coefficients[1])
m_giss = as.numeric(model_giss$coefficients[2])
i_giss = -b_giss/m_giss
dev_fit_giss = as.numeric(deviance(model_giss))
value_giss_frame = data.frame(gith_x$n_step, predict(model_giss, list(n_step = gith_x$n_step), type = 'response'))
value_giss = value_giss_frame[[2]]
new3 = c(sub, "giss", temp_group, temp_age, temp_com_cond, m_giss, i_giss, value_giss, dev_fit_giss)
d_glm_fit[index, ] = new3
}
d_glm_fit$slope_coef = as.numeric(d_glm_fit$slope_coef)
d_glm_fit$inflection_step = as.numeric(d_glm_fit$inflection_step)
d_glm_fit$deviance = as.numeric(d_glm_fit$deviance)
d_glm_fit$word<-as.factor(d_glm_fit$word)
d_glm_fit$group<-as.factor(d_glm_fit$group)
d_glm_fit$age<-as.factor(d_glm_fit$age)
View(d_conts)
View(d_conts)
##bind inflection to be within range for  steps
sum(d_glm_fit$inflection_step < 1 |d_glm_fit$inflection_step > 7) #31/288 (11%) #what percentage out of bounds?
d_glm_fit<-d_glm_fit %>% filter(inflection_step > 0 & inflection_step < 8)
View(d_glm_fit)
names(d_glm_fit)
write.csv(d_conts,"per_g_for_modeling.csv")
d_glm_all = d_glm_fit %>%
dplyr::group_by(PartID, group,com_cond) %>%
dplyr::summarise(mean = mean(slope_coef_t,inflection_step, na.rm = T))
d_glm_fit$slope_coef_t<-as.numeric(abs(d_glm_fit$slope_coef))
d_glm_fit$slope_coef_t<-log10(d_glm_fit$slope_coef_t)
d_glm_all = d_glm_fit %>%
dplyr::group_by(PartID, group,com_cond) %>%
dplyr::summarise(mean = mean(slope_coef_t,inflection_step, na.rm = T))
d_glm_all = d_glm_fit %>%
dplyr::group_by(PartID, group,com_cond) %>%
dplyr::summarise(slope = mean(slope_coef_t, na.rm = T),inflection=mean(inflection_step,na.rm=T))
View(d_glm_all)
d_glm_all = d_glm_fit %>%
dplyr::group_by(com_cond) %>%
dplyr::summarise(slope = mean(slope_coef_t, na.rm = T),inflection=mean(inflection_step,na.rm=T))
View(d_glm_all)
d_glm_all = d_glm_fit %>%
dplyr::group_by(group,com_cond) %>%
dplyr::summarise(slope = mean(slope_coef_t, na.rm = T),inflection=mean(inflection_step,na.rm=T))
d_glm_all = d_glm_fit %>%
dplyr::group_by(word,com_cond) %>%
dplyr::summarise(slope = mean(slope_coef_t, na.rm = T),inflection=mean(inflection_step,na.rm=T))
View(d_glm_all)
write.csv(d_glm_all,"slope_inflection_modeling.csv")
